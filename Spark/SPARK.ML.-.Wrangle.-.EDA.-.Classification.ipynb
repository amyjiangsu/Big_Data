{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Extraction and Machine Learning in Spark\n",
    "\n",
    "**Challenge**:  Predict who will surive the Titanic Disaster.\n",
    "\n",
    "At times, the only meta-data about a given data source is domain knowledge.  In this case, here is a high-level overview of the configuration of the file. All other information such as source and schema is entirely missing.  In this case, the data is a csv file with the following information available:\n",
    "\n",
    "**Passenger ID**: Unique ID of that passenger\n",
    "\n",
    "**Survived**: Whether or not that passenger survived or perished (1 == survived, 0 == did not survive).\n",
    "\n",
    "**PClass**: Passenger class a proxy for socio-economic status (SES) 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower\n",
    "\n",
    "**Sex**: Gender\n",
    "\n",
    "**Age** is in Years: Fractional if Age less than One (1) f the Age is Estimated, it is in the form xx.5\n",
    "\n",
    "**Sibsp**: Number of Siblings/Spouses Aboard\n",
    "\n",
    "**Parch**: Number of Parents/Children Aboard\n",
    "\n",
    "**Embarked**: Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "#### Load the libraries\n",
    "You'll notice that there are a lot of libraries loaded for this example.  That is because of the very broad range of functions is needed to cover the entire data pipelining process.  The first block of libraries is for data loading, wrangling, and EDA.  While the second is for machine learning and diagnostics.  After the libraries have been loaded, a Spark Context is created connecting the local instance (driver) to the hypothetical Spark Cluster (executor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#Load all the libraries\n",
    "#Spark envrionment and configuration\n",
    "import getspark #Load environment\n",
    "from pyspark import SparkContext  #Spark Context Local Driver\n",
    "from pyspark.sql import SQLContext #SQL Context Local Driver\n",
    "from pyspark.ml.feature import StringIndexer #Converts strings to indexes\n",
    "from pyspark.sql.types import * #All Spark SQL data types\n",
    "from pyspark.sql import Row #Row configurations\n",
    "from pyspark.sql.functions import col, when, udf\n",
    "\n",
    "#Machine Learning Libraries\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel, LogisticRegressionModel, LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Environment Setup and instantiation\n",
    "sc = SparkContext() #Load spark context\n",
    "sqlContext = SQLContext(sc) #Load SQL context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data as a text 'blob'\n",
    "After loading in the data in as an RDD, we can take a look at it to view some basic attributes.  The first row of the data is in alignment with the metadata descriptions provided, along with seemingly consistent patterns within each follow-up row.  The next step involves splitting the data by its delimiter, which is a perceived to be comma.  Afterwards, the first row containing the header information will need to be removed.  \n",
    "\n",
    "The following steps involve creating a dataframe object based on the data with the header included.  This dataframe will then be cached into memory because we will be exploring it and iterating over it several times.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(r\"C:\\Spark\\train.csv\") #Load in the data as a rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'PassengerId,Survived,Pclass,Sex,Age,SibSp,Parch,Embarked',\n",
       " u'1,0,3,male,22,1,0,S',\n",
       " u'2,1,1,female,38,1,0,C',\n",
       " u'3,1,3,female,26,0,0,S',\n",
       " u'4,1,1,female,35,1,0,S']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5) #check out the first five lines of the rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "892"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count() #print out the total number of lines in the rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'PassengerId',\n",
       " u'Survived',\n",
       " u'Pclass',\n",
       " u'Sex',\n",
       " u'Age',\n",
       " u'SibSp',\n",
       " u'Parch',\n",
       " u'Embarked']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split it by its delimiter\n",
    "rdd = rdd.map(lambda line: line.split(\",\")) #Split it up by comma -transformation\n",
    "\n",
    "#Strip out the header\n",
    "header = rdd.first() #Extract header\n",
    "data = rdd.filter(lambda x:x !=header) #Create a headerless rdd\n",
    "header #View the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: string, Survived: string, Pclass: string, Sex: string, Age: string, SibSp: string, Parch: string, Embarked: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edaframe = sqlContext.createDataFrame(data, header) #create a dataframe from the rdd\n",
    "edaframe.cache() #cache the dataframe in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review our data \n",
    "\n",
    "The following lines of code perform some basic diagnostics on the dataframe.  We now have visibility as to the schema and formalization of the data.  Now it is up to us to check to make sure our assumptions about the data align with the actual content contain within.  \n",
    "\n",
    "Note: that the data is import as strings by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schema of this data is a follows:\n",
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SibSp: string (nullable = true)\n",
      " |-- Parch: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "Here is a sample of the top five rows:\n",
      "+-----------+--------+------+------+---+-----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|   Sex|Age|SibSp|Parch|Embarked|\n",
      "+-----------+--------+------+------+---+-----+-----+--------+\n",
      "|          1|       0|     3|  male| 22|    1|    0|       S|\n",
      "|          2|       1|     1|female| 38|    1|    0|       C|\n",
      "|          3|       1|     3|female| 26|    0|    0|       S|\n",
      "|          4|       1|     1|female| 35|    1|    0|       S|\n",
      "|          5|       0|     3|  male| 35|    0|    0|       S|\n",
      "+-----------+--------+------+------+---+-----+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The total number of rows equals 891\n"
     ]
    }
   ],
   "source": [
    "print \"The schema of this data is a follows:\"\n",
    "edaframe.printSchema()\n",
    "print \"Here is a sample of the top five rows:\"\n",
    "edaframe.show(5)\n",
    "ct = edaframe.count()\n",
    "print \"The total number of rows equals %s\" %ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View associations by grouping the various data types together\n",
    "Working with factor level data is very common, the following lines of code further explore the data by performing a series of group-wise comparisons for each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       0|  549|\n",
      "|       1|  342|\n",
      "+--------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|Pclass|count|\n",
      "+------+-----+\n",
      "|     1|  216|\n",
      "|     2|  184|\n",
      "|     3|  491|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|   Sex|count|\n",
      "+------+-----+\n",
      "|female|  314|\n",
      "|  male|  577|\n",
      "+------+-----+\n",
      "\n",
      "+----+-----+\n",
      "| Age|count|\n",
      "+----+-----+\n",
      "|70.5|    1|\n",
      "|36.5|    1|\n",
      "|  50|   10|\n",
      "|  51|    7|\n",
      "|0.75|    2|\n",
      "|  52|    6|\n",
      "|  53|    1|\n",
      "|  54|    8|\n",
      "|55.5|    1|\n",
      "|  55|    2|\n",
      "|  56|    4|\n",
      "|  57|    2|\n",
      "|  58|    5|\n",
      "|40.5|    2|\n",
      "|  59|    2|\n",
      "|45.5|    2|\n",
      "|30.5|    2|\n",
      "|20.5|    1|\n",
      "|   1|    7|\n",
      "|   2|   10|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+-----+\n",
      "|SibSp|count|\n",
      "+-----+-----+\n",
      "|    0|  608|\n",
      "|    1|  209|\n",
      "|    2|   28|\n",
      "|    3|   16|\n",
      "|    4|   18|\n",
      "|    5|    5|\n",
      "|    8|    7|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|Parch|count|\n",
      "+-----+-----+\n",
      "|    0|  678|\n",
      "|    1|  118|\n",
      "|    2|   80|\n",
      "|    3|    5|\n",
      "|    4|    4|\n",
      "|    5|    5|\n",
      "|    6|    1|\n",
      "+-----+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|Embarked|count|\n",
      "+--------+-----+\n",
      "|       C|  168|\n",
      "|       Q|   77|\n",
      "|       S|  644|\n",
      "|        |    2|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edaframe.groupBy(\"Survived\").count().show()\n",
    "edaframe.groupBy(\"Pclass\").count().show()\n",
    "edaframe.groupBy(\"Sex\").count().show()\n",
    "edaframe.groupBy(\"Age\").count().show()\n",
    "edaframe.groupBy(\"SibSp\").count().show()\n",
    "edaframe.groupBy(\"Parch\").count().show()\n",
    "edaframe.groupBy(\"Embarked\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the data types appear to be categorical or discrete data factorized as categorical.  The one exception appears to be age, which is assumed to be continuous.  Also worth noting is that the count of blanks on the Embarked column.  This indicates that there are missing values which are not coded as 'null' within the dataframe.  It does not appear this happens on any of the other columns, except for maybe Age.  To understand this better, we will conduct a frequency count of the Age data to check for the pressence of missing or incomplete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Age_freqItems=[u'28', u'25', u''])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Frequency counts by the Age group\n",
    "freqcount = edaframe.freqItems(['Age'], 0.1).collect()\n",
    "freqcount[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the frequency count analysis, we can see that there are a lot of blank values in the dataframe.  These are represented as a space.  These empty spaces will prevent the data frame from being converted to integers or floats because it is still a string at this point.  We need to find each blank space and replace it with a Null placeholder.  One of the more powerful utilities of Spark is to be able to create UDFs and apply them to the source data.  We will do this in the following example where a new UDF know as 'blank_as_null' will be created to replace all the blank values with a *'None'* placeholder\n",
    "\n",
    "Recap Note: In Spark dataframes, Null is represented by None.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lot of empty strings in age we need to replace them with null values\n",
    "def blank_as_null(x):\n",
    "    return when(col(x) != \"\", col(x)).otherwise(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------+----+-----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|   Sex| Age|SibSp|Parch|Embarked|\n",
      "+-----------+--------+------+------+----+-----+-----+--------+\n",
      "|          1|       0|     3|  male|  22|    1|    0|       S|\n",
      "|          2|       1|     1|female|  38|    1|    0|       C|\n",
      "|          3|       1|     3|female|  26|    0|    0|       S|\n",
      "|          4|       1|     1|female|  35|    1|    0|       S|\n",
      "|          5|       0|     3|  male|  35|    0|    0|       S|\n",
      "|          6|       0|     3|  male|null|    0|    0|       Q|\n",
      "|          7|       0|     1|  male|  54|    0|    0|       S|\n",
      "|          8|       0|     3|  male|   2|    3|    1|       S|\n",
      "|          9|       1|     3|female|  27|    0|    2|       S|\n",
      "|         10|       1|     2|female|  14|    1|    0|       C|\n",
      "+-----------+--------+------+------+----+-----+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edaframenoempty = edaframe.withColumn(\"Age\", blank_as_null(\"Age\")) #loop over the Age column and replace blanks\n",
    "edaframenoempty = edaframenoempty.withColumn(\"Embarked\", blank_as_null(\"Embarked\"))  #same with the Embarked column\n",
    "edaframenoempty.show(10) #Check out the new data frame                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Age_freqItems=[u'28', u'25', None])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqcount = edaframenoempty.freqItems(['Age'], 0.1).collect() #Redo the frequency count to make sure that '' is no longer present\n",
    "freqcount[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edaframenoempty.count() #Count the dataframe to make sure no data was lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis\n",
    "\n",
    "Now that the data is in reasonable shape, it is time to begin exporatory data analysis in earnest.  We will begin with some desriptive statistics.  This includes the five number summary and counting the number of Null values in that data set.  We can see that omitting the null values results in a loss of a sizable amount of data.  Given the integrity of these values, then next best step is to impute the missing values to maintain as many observations as possible.  In this example we will use a simple average as the imputed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               Age|\n",
      "+-------+------------------+\n",
      "|  count|               712|\n",
      "|   mean| 29.64209269662921|\n",
      "| stddev|14.482751702789983|\n",
      "|    min|              0.42|\n",
      "|    max|                 9|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edaframenoempty.na.drop().describe('Age').show() #descriptive statistics, note that you have to drop NAs for this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edadfimputed = edaframenoempty.na.fill({'Age' : 30})\n",
    "edadfimputed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "To be able to perform mathematical operations and modeling, we need to convert text data into ints or floats.  This borrows heavily from tokenizing practices.  For a simple example, we'll begin by creating another UDF for converting the text identifiers for gender into a boolean value. We will also force a schema of our design onto the dataframe with the Row function.  This will allow us to explicity state the schema, and avoid having to tokenize data types later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genmap(gender):\n",
    "    if gender == 'female':\n",
    "        return 1 #female\n",
    "    else:\n",
    "        return 0 #Not Female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfmappd = edadfimputed.map(lambda line: Row(passenger_id = str(line[0]), \n",
    "                                    survived = int(line[1]), \n",
    "                                    pclass = int(line[2]), \n",
    "                                    sex = genmap(line[3]),\n",
    "                                    age = float(line[4]),\n",
    "                                    sibsp = int(line[5]),\n",
    "                                    parch = int(line[6]),\n",
    "                                    embarked=str(line[7]))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The schema of this data is a follows:\n",
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- embarked: string (nullable = true)\n",
      " |-- parch: long (nullable = true)\n",
      " |-- passenger_id: string (nullable = true)\n",
      " |-- pclass: long (nullable = true)\n",
      " |-- sex: long (nullable = true)\n",
      " |-- sibsp: long (nullable = true)\n",
      " |-- survived: long (nullable = true)\n",
      "\n",
      "Here is a sample of the top five rows:\n",
      "+----+--------+-----+------------+------+---+-----+--------+\n",
      "| age|embarked|parch|passenger_id|pclass|sex|sibsp|survived|\n",
      "+----+--------+-----+------------+------+---+-----+--------+\n",
      "|22.0|       S|    0|           1|     3|  0|    1|       0|\n",
      "|38.0|       C|    0|           2|     1|  1|    1|       1|\n",
      "|26.0|       S|    0|           3|     3|  1|    0|       1|\n",
      "|35.0|       S|    0|           4|     1|  1|    1|       1|\n",
      "|35.0|       S|    0|           5|     3|  0|    0|       0|\n",
      "+----+--------+-----+------------+------+---+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The total number of rows equals 891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[age: double, embarked: string, parch: bigint, passenger_id: string, pclass: bigint, sex: bigint, sibsp: bigint, survived: bigint]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out the new dataframe and punch it into memory for QA\n",
    "print \"The schema of this data is a follows:\"\n",
    "dfmappd.printSchema()\n",
    "print \"Here is a sample of the top five rows:\"\n",
    "dfmappd.show(5)\n",
    "ct = dfmappd.count()\n",
    "print \"The total number of rows equals %s\" %ct\n",
    "dfmappd.cache() #cache the new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+---+---+\n",
      "|survived_pclass|  1|  2|  3|\n",
      "+---------------+---+---+---+\n",
      "|              1|136| 87|119|\n",
      "|              0| 80| 97|372|\n",
      "+---------------+---+---+---+\n",
      "\n",
      "+-----------------+---+---+----+---+\n",
      "|survived_embarked|  Q|  S|None|  C|\n",
      "+-----------------+---+---+----+---+\n",
      "|                1| 30|217|   2| 93|\n",
      "|                0| 47|427|   0| 75|\n",
      "+-----------------+---+---+----+---+\n",
      "\n",
      "+------------+---+---+\n",
      "|survived_sex|  0|  1|\n",
      "+------------+---+---+\n",
      "|           1|109|233|\n",
      "|           0|468| 81|\n",
      "+------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# categorical/boolean fields can give valuable facets (crosstabs)\n",
    "dfmappd.crosstab('survived', 'pclass').show()\n",
    "dfmappd.crosstab('survived', 'embarked').show()\n",
    "dfmappd.crosstab('survived', 'sex').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be costly to have to write UDFs for every categorical variable, this also could result in a lot of technological debt, maintanence, and fragility down the line.  Thankfull Spark has numerous tokenizer functions available for us to use.  We'll apply a common one known as StringIndexer to the embarked column to make a new column for the tokenized values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------------+------+---+-----+--------+----------+\n",
      "| age|embarked|parch|passenger_id|pclass|sex|sibsp|survived|embarkerdv|\n",
      "+----+--------+-----+------------+------+---+-----+--------+----------+\n",
      "|22.0|       S|    0|           1|     3|  0|    1|       0|       0.0|\n",
      "|38.0|       C|    0|           2|     1|  1|    1|       1|       1.0|\n",
      "|26.0|       S|    0|           3|     3|  1|    0|       1|       0.0|\n",
      "|35.0|       S|    0|           4|     1|  1|    1|       1|       0.0|\n",
      "|35.0|       S|    0|           5|     3|  0|    0|       0|       0.0|\n",
      "|30.0|       Q|    0|           6|     3|  0|    0|       0|       2.0|\n",
      "|54.0|       S|    0|           7|     1|  0|    0|       0|       0.0|\n",
      "| 2.0|       S|    1|           8|     3|  0|    3|       0|       0.0|\n",
      "|27.0|       S|    2|           9|     3|  1|    0|       1|       0.0|\n",
      "|14.0|       C|    0|          10|     2|  1|    1|       1|       1.0|\n",
      "+----+--------+-----+------------+------+---+-----+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"embarked\", outputCol=\"embarkerdv\")\n",
    "indexed = indexer.fit(dfmappd).transform(dfmappd)\n",
    "indexed.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|embarkerdv|count|\n",
      "+----------+-----+\n",
      "|       1.0|  168|\n",
      "|       3.0|    2|\n",
      "|       0.0|  644|\n",
      "|       2.0|   77|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed.groupby('embarkerdv').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|               891|\n",
      "|   mean|29.758888888888887|\n",
      "| stddev|12.995271375176369|\n",
      "|    min|              0.42|\n",
      "|    max|              80.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed.describe('age').show() #Revisiting age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With continuous data, it is a good practice to visualize it with a histogram.  This is important to understand the variance and distribution around the data, and if certain assumptions can be maintained.  Spark does not have a native histogram function available for dataframes yet.  We'll have to hack one together by grouping and counting the data and then coercing it into a Pandas dataframe to create the visualization.  Once this is done, we can see that the data follows a normal-ish distribution, with an abundence of values around thirty, which is our imputed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def spark_histogram(df, column):\n",
    "    counts = df.groupby(column).count()\n",
    "    df = counts.toPandas()\n",
    "    df[column] = df.age.astype(float) #Specify the column here\n",
    "    return df.sort_values(column).set_index(column).iloc[:100,:].plot(kind='bar', figsize=(14,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x93ad410>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzMAAAFTCAYAAAANwFLFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x94f5ab0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark_histogram(dfmappd, 'age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning with Spark\n",
    "\n",
    "With EDA and data wrangling out of the way, it is time to begin the machine learning exercise in earnest.  The goals need to be to create a model that predicts the outcome (survival) with a reasonable degree of precision and accuracy.  The model also has to be applied to a new dataframe that can be exported for futher use.  At this time, dataframes cannot be used to create models, word is that this will happen in a future release of Spark.  Until then, we will need to rely on data format called a 'labeledpoint' to conduct modeling.  The label point is a tuple consisting of a response value Y and a series of predictor variables X1..X2..X3..Xn.  Spark has a build in Vector assembler function that will help us with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\", \"parch\", \"pclass\", \"sex\", \"embarkerdv\", \"sibsp\"],\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed = assembler.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------------+------+---+-----+--------+----------+--------------------+\n",
      "| age|embarked|parch|passenger_id|pclass|sex|sibsp|survived|embarkerdv|            features|\n",
      "+----+--------+-----+------------+------+---+-----+--------+----------+--------------------+\n",
      "|22.0|       S|    0|           1|     3|  0|    1|       0|       0.0|[22.0,0.0,3.0,0.0...|\n",
      "|38.0|       C|    0|           2|     1|  1|    1|       1|       1.0|[38.0,0.0,1.0,1.0...|\n",
      "|26.0|       S|    0|           3|     3|  1|    0|       1|       0.0|[26.0,0.0,3.0,1.0...|\n",
      "|35.0|       S|    0|           4|     1|  1|    1|       1|       0.0|[35.0,0.0,1.0,1.0...|\n",
      "|35.0|       S|    0|           5|     3|  0|    0|       0|       0.0|(6,[0,2],[35.0,3.0])|\n",
      "+----+--------+-----+------------+------+---+-----+--------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed.show(5) #Not the new features column which is a dense vector of the predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- embarked: string (nullable = true)\n",
      " |-- parch: long (nullable = true)\n",
      " |-- passenger_id: string (nullable = true)\n",
      " |-- pclass: long (nullable = true)\n",
      " |-- sex: long (nullable = true)\n",
      " |-- sibsp: long (nullable = true)\n",
      " |-- survived: long (nullable = true)\n",
      " |-- embarkerdv: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed.printSchema() #Confirm the dense vector data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the labeled point datatype\n",
    "lpdata = transformed.select(col(\"survived\"), col(\"features\")).map(lambda row: LabeledPoint(row.survived, row.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [22.0,0.0,3.0,0.0,0.0,1.0]),\n",
       " LabeledPoint(1.0, [38.0,0.0,1.0,1.0,1.0,1.0]),\n",
       " LabeledPoint(1.0, [26.0,0.0,3.0,1.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [35.0,0.0,1.0,1.0,0.0,1.0]),\n",
       " LabeledPoint(0.0, (6,[0,2],[35.0,3.0])),\n",
       " LabeledPoint(0.0, [30.0,0.0,3.0,0.0,2.0,0.0]),\n",
       " LabeledPoint(0.0, (6,[0,2],[54.0,1.0])),\n",
       " LabeledPoint(0.0, [2.0,1.0,3.0,0.0,0.0,3.0]),\n",
       " LabeledPoint(1.0, [27.0,2.0,3.0,1.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [14.0,0.0,2.0,1.0,1.0,1.0])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpdata.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning\n",
    "\n",
    "In the following example, we will use a classic machine learning algorithm called logistic regression.  Logistic regression has been around since the 1950s and works by coercing a series of linear predictors into a sigmoid function against a binary dependent variable.  After the model has been run, we will check for a variety of diagnostics, such as Error Rate and Area under the ROC Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logstmodel = LogisticRegressionWithLBFGS.train(lpdata) #Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.0059, 0.0038, -0.6247, 2.5438, 0.3116, -0.1849])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logstmodel.weights #AKA coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Training Error = 0.204264870932\n",
      "Area under ROC = 0.772505033074\n"
     ]
    }
   ],
   "source": [
    "# Compute raw scores on the training set\n",
    "predictionAndLabels = lpdata.map(lambda lp: (float(logstmodel.predict(lp.features)), lp.label))\n",
    "labelsAndPreds = lpdata.map(lambda p: (p.label, logstmodel.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(lpdata.count())\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "print(\"Logistic Regression Training Error = \" + str(trainErr))\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring\n",
    "\n",
    "Now that we are satified with our model, we need to save the scores for future usage.  This involves mapping the completed model over a collection of features in the dataframe.  Afterwards, the newly completed scores data frame can be saved back to hadoop or exported to a different location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valuesAndPreds = transformed.map(lambda row: (row.passenger_id, row.survived, float(logstmodel.predict(row.features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfmappd2 = valuesAndPreds.map(lambda line: Row(passenger_id = int(line[0]),\n",
    "                                               survived = int(line[1]),\n",
    "                                               score = float(line[2]))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdframe = dfmappd2.toPandas() #ship the predictions to a pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_id</th>\n",
       "      <th>score</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   passenger_id  score  survived\n",
       "0             1      0         0\n",
       "1             2      1         1\n",
       "2             3      1         1\n",
       "3             4      1         1\n",
       "4             5      0         0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdframe.head() #Now we can export it, write it to a csv etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Scoring Process\n",
    "\n",
    "Assuming that the model was built in an external process, how can we create scores based on the coefficients of our external model?  This would involve creating a user defined function that passed the scores to a new DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myLogisticFunc(age, parch, pclass):\n",
    "    intercept = 3.435222\n",
    "    betaAge = -0.039841\n",
    "    betaParch = 0.176439\n",
    "    betaPclass = -1.239452\n",
    "    z = intercept + betaAge * age + betaParch * parch + betaPclass * pclass\n",
    "    return 1.0 / (1.0 + math.exp(-z))\n",
    "myLogisticFuncUDF = udf(myLogisticFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|passenger_id|              score|\n",
      "+------------+-------------------+\n",
      "|           1|0.23873343214582232|\n",
      "|           2|  0.664142996966085|\n",
      "|           3| 0.2109847155307048|\n",
      "|           4| 0.6902599781234785|\n",
      "|           5|0.15741764175944903|\n",
      "|           6|0.18567465460884552|\n",
      "|           7| 0.5110871822654997|\n",
      "|           8| 0.4535401594510565|\n",
      "|           9|0.26777064098514214|\n",
      "|          10| 0.5983377902559295|\n",
      "+------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed.withColumn(\"score\", myLogisticFuncUDF(col(\"age\"), col(\"parch\"), col(\"pclass\"))).select(\"passenger_id\", \"score\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
