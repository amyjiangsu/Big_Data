{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frames With Spark & Pyspark\n",
    "\n",
    "**Summary**\n",
    "\n",
    "**Motivation**example I will be going over a small use case of the Data Science Pipline involving Apache Spark.  The goal is to be able to create predicitions at scale for whether users will click on an add or not.  \n",
    "\n",
    "**GLOSSARY:**\n",
    "\n",
    "**Show** Shows the content of the data frame, accepts an argument of how many rows to show.  Show is an *action*\n",
    "* **dataframe.show**(*int*)\n",
    "\n",
    "**dataframe.printSchema**( ) prints the schema of the data frame in tree format\n",
    "\n",
    "**dataframe.select**(*column_name*) selects the column based on the column name \n",
    "\n",
    "**dataframe.filter**(dataframe['*column_name*] *logical argument*) filters the dataframe based on column id, and logical criteria. \n",
    "\n",
    "**dataframe.groupBy**(*column_name*) groups records in the rdd by \n",
    "\n",
    "**dataframe.drop**(*column_name*) drop columns in the dataframe by the column name string - transformation\n",
    "\n",
    "**dataframe.dropDuplicates**([*column1_name*,*column2_name*, ...]) drop columns in the dataframe by the column name string - transformation\n",
    "\n",
    "**dataframe.dropna**([*column1_name*,*column2_name*, ...]) drop columns in the dataframe by the column name string - transformation\n",
    "\n",
    "**dataframe.dtypes**([*column1_name*,*column2_name*, ...]) drop columns in the dataframe by the column name string - transformation\n",
    "\n",
    "**dataframe.fillna**([*column1_name*,*column2_name*, ...]) drop columns in the dataframe by the column name string - transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import getspark\n",
    "from IPython.display import Image\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will parse through the data and turn the first column into a LabeledPoint.  Next, read in the text file, parse it, and apply the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load and parse the data \n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(',')]\n",
    "    return LabeledPoint(values[0], values[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read in the data (Transform)\n",
    "2. Take the first five rows of the data (Action)\n",
    "3. Map the data by spliting each of the rows by comma since its a CSV (Action)\n",
    "4. Sample 10000 rows from the data\n",
    "5. Write the sampled rdd to a csv to use in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(r\"C:\\Spark\\clickinfo.csv\") # Read in the data -transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'user_id,clicks,impression,signed_in',\n",
       " u'1,0,3,1',\n",
       " u'2,0,3,1',\n",
       " u'3,0,3,1',\n",
       " u'4,0,3,1']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5) #Take the first five rows to check it out - action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = data.map(lambda line: line.split(\",\")) #split it up by comma -transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'user_id', u'clicks', u'impression', u'signed_in'],\n",
       " [u'1', u'0', u'3', u'1'],\n",
       " [u'2', u'0', u'3', u'1'],\n",
       " [u'3', u'0', u'3', u'1'],\n",
       " [u'4', u'0', u'3', u'1']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5) #Check out the first five rows -action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rdd2 = rdd.sample(False, 0.6) #Sample 10000 rows from the data -transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rdd2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = rdd.first() #extract header\n",
    "data = rdd.filter(lambda x:x !=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = data.map(lambda line: Row(clicks = line[0], \n",
    "                              gender = line[1], impression=line[2], \n",
    "                              signedin=line[3])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+--------+\n",
      "|clicks|gender|impression|signedin|\n",
      "+------+------+----------+--------+\n",
      "|     1|     0|         3|       1|\n",
      "|     2|     0|         3|       1|\n",
      "|     3|     0|         3|       1|\n",
      "|     4|     0|         3|       1|\n",
      "|     5|     0|        11|       1|\n",
      "+------+------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- clicks: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- impression: string (nullable = true)\n",
      " |-- signedin: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+--------+\n",
      "|clicks|gender|impression|signedin|\n",
      "+------+------+----------+--------+\n",
      "|     5|     0|        11|       1|\n",
      "|     6|     1|        11|       1|\n",
      "|    72|     0|        10|       1|\n",
      "|   141|     0|        11|       0|\n",
      "|   186|     0|        10|       1|\n",
      "+------+------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[df.impression >= 10].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|gender|impression|\n",
      "+------+----------+\n",
      "|     0|         3|\n",
      "|     0|         3|\n",
      "|     0|         3|\n",
      "|     0|         3|\n",
      "|     0|        11|\n",
      "+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('gender','impression').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group By and count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     0|56607|\n",
      "|     1| 5499|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"gender\").count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel, LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = df.map(lambda line:LabeledPoint(line[0],[line[1:]]))\n",
    "temp.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingData, testingData = temp.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "svmmodel = SVMWithSGD.train(temp, iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp.map(lambda p: (svmmodel.predict(p.features))).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictionAndLabels = temp.map(lambda lp: (float(svmmodel.predict(lp.features)), lp.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictionAndLabels.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainErr = prediObserRDDin.filter(lambda (v, p): v != p).count() / float(temp.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svmmodel.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svmmodel.predict([1.0, 11.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsedData = rdd.map(parsePoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logmodel = LogisticRegressionWithLBFGS.train(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelsAndPreds = data.map(lambda p: (p.label, svmmodel.predict(p.features)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelsAndPreds = temp.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(parsedData.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelsAndPreds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionAndLabelslog = parsedData.map(lambda lp: (float(logmodel.predict(lp.features)), lp.label))\n",
    "predictionAndLabelssvm = parsedData.map(lambda lp: (float(svmmodel.predict(lp.features)), lp.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics = BinaryClassificationMetrics(predictionAndLabelslog)\n",
    "print(\"Logistic - Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics = BinaryClassificationMetrics(predictionAndLabelssvm)\n",
    "print(\"SVM - Area under ROC = %s\" % metrics.areaUnderROC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save and load model\n",
    "model.save(sc, \"myModelPath\")\n",
    "sameModel = SVMModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics = BinaryClassificationMetrics(predictionAndLabelsrf)\n",
    "print(\"RF - Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsedData = data.map(parsePoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsedData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
