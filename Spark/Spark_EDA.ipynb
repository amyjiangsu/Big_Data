{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conducting EDA in Spark\n",
    "\n",
    "EDA is a critical part of the data science process, in this example I will show you how to \n",
    "\n",
    "1. Read in the data\n",
    "2. Transform it to a functional state\n",
    "3. Conduct EDA (five num summary, grouping, crosstabs) \n",
    "4. Create a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import getspark\n",
    "from IPython.display import Image\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(r\"C:\\Spark\\clickinfo.csv\")\n",
    "#Split it by its delimiter\n",
    "rdd = rdd.map(lambda line: line.split(\",\")) #split it up by comma -transformation\n",
    "#Strip out the header\n",
    "header = rdd.first() #extract header\n",
    "data = rdd.filter(lambda x:x !=header) #review the headerless rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to \n",
    "def signedin(clicks):\n",
    "    if clicks == '0':\n",
    "        return \"Not_Signed_In\"\n",
    "    else: \n",
    "        return \"Signed_In\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clicksmappd = data.map(lambda line: Row(user_id = str(line[0]), \n",
    "                              clicks = int(line[1]), \n",
    "                              impression=int(line[2]), \n",
    "                              signedin=signedin(line[3]))).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA One:  Looking up the frequency of events\n",
    "\n",
    "It is possible to approximate the frequecy that something occurs.  This can be done using the **.freqItems()** argument.  Note that this algorithm is an approximation, and may produce some false positives.  In this count we see if people are more often signed in or out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqcount = clicksmappd.freqItems(['signedin'], 0.7).collect() #0.7 is the frequency proportion (minimum proportion of rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqcount[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA Two: Grouping and summarizing\n",
    "\n",
    "Spark dataframes allow you to group and summarize data the same way you would with pandas data frames.  Another useful function is to create cross tabs, which transforms the dataframe from long to wide.  Crosstabs show what values occur in what columns related to one another.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clicksmappd.groupby(['clicks', 'signedin']).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clicksmappd.crosstab('clicks', 'signedin').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Three: The five number summary\n",
    "\n",
    "Spark can create summary statistics from dataframes as well.  This is accomplished using the **.describe()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clicksmappd.describe('clicks','impression').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#funtion to make a histogram, kind(s) include: 'bar', 'box', and 'density\n",
    "def spark_histogram(df, column):\n",
    "    counts = df.groupby(column).count()\n",
    "    df = counts.toPandas()\n",
    "    df[column] = df.impression.astype(float) #Specify the column here\n",
    "    return df.sort_values(column).set_index(column).iloc[:50,:].plot(kind='bar', figsize=(14,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_histogram(clicksmappd, 'impression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# massive outliers, will skew histogram buckets\n",
    "no_out_df = clicksmappd.filter(clicksmappd['impression'] < 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_histogram(no_out_df, 'impression')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
