{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "In this example, we will demonstrate interactive data analysis using text files, Spark SQL, dataframes, and functional programming.  Spark SQL jobs will typically involved a few key steps, these include:\n",
    "\n",
    "1. Connect to the environment\n",
    "2. Create a Spark Context\n",
    "3. Create a Spark SQL context\n",
    "4. Read in the file to be analyzed\n",
    "5. Transform it into a data frame\n",
    "6. Begin to query that dataframe using Spark SQL.\n",
    "\n",
    "Detailed documentation can be found on the Apache website [here:](http://spark.apache.org/docs/latest/sql-programming-guide.html) - note: this is for Spark 1.5.2\n",
    "\n",
    "Lastly - when using Hive the best practice would be to avoid the 4th and 5th steps by connecting to Hive tables directly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import getspark\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "sc = SparkContext( 'local', 'pyspark')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with previous examples, we will begin by reading in the text file for the click data.  This version is in CSV format.  After it is read in as an rdd, the next step is to split it by its delimiter, then remove the header and check out the finalized and headerless rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define the lineage graph for the rdd from the clickinfo text file\n",
    "rdd = sc.textFile(r\"C:\\Spark\\clickinfo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split it by its delimiter\n",
    "rdd = rdd.map(lambda line: line.split(\",\")) #split it up by comma -transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Strip out the header\n",
    "header = rdd.first() #extract header\n",
    "data = rdd.filter(lambda x:x !=header) #review the headeress rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'1', u'0', u'3', u'1'],\n",
       " [u'2', u'0', u'3', u'1'],\n",
       " [u'3', u'0', u'3', u'1'],\n",
       " [u'4', u'0', u'3', u'1'],\n",
       " [u'5', u'0', u'11', u'1']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out your fancy new rdd\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why go through the trouble?\n",
    "\n",
    "* Schema = table + columns + types\n",
    "* Column names to index\n",
    "* Leverage SQL and relational theory\n",
    "\n",
    "### Why not schemas and SQL?\n",
    "\n",
    "* They make your data structure\n",
    "* Fragility\n",
    "\n",
    "When reviewing our schema, one of the columns has a boolean for male == 1 and female == 0.  To ease interpretability for reporting, we can find a replace the values as male and female respectively.  Note: currently only strings exist in the rdd, in the next step they will be redefined as specific ints, floats, strings etc.  The row function with Spark dataframes is helpful, becaulse it allows you to programmatically define the schema up front.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Instead of reading in the sql context directly, we can define the rows uniquely in the dataframe and define the schema\n",
    "def genmap(gender):\n",
    "    if gender == '0':\n",
    "        return \"Female\"\n",
    "    else:\n",
    "        return \"male\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfmappd = data.map(lambda line: Row(clicks = int(line[0]), \n",
    "                              gender = genmap(line[1]), impression=int(line[2]), \n",
    "                              signedin=int(line[3]))).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with our dataframe created, we can review the schema and take a topline sample of the dataframe.  Once we have confirmed that the schema matches our desired format the next step is to register a temporary table to be able to we can then execute SQL queries against the data as you would with any normal SQL query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- clicks: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- impression: long (nullable = true)\n",
      " |-- signedin: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfmappd.printSchema() # This defines the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+--------+\n",
      "|clicks|gender|impression|signedin|\n",
      "+------+------+----------+--------+\n",
      "|     1|Female|         3|       1|\n",
      "|     2|Female|         3|       1|\n",
      "|     3|Female|         3|       1|\n",
      "|     4|Female|         3|       1|\n",
      "|     5|Female|        11|       1|\n",
      "+------+------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfmappd.show(5) # Shows a snippet of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfmappd.registerTempTable(\"clickinfo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----------+\n",
      "|gender|clicks|impressions|\n",
      "+------+------+-----------+\n",
      "|Female| 16063|         18|\n",
      "|Female| 31110|         17|\n",
      "|Female| 56240|         16|\n",
      "|  male| 25224|         16|\n",
      "|Female| 51154|         16|\n",
      "+------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"\"\"SELECT gender, clicks, SUM(impression) as impressions \n",
    "                  FROM clickinfo \n",
    "                  GROUP BY gender, clicks \n",
    "                  ORDER BY SUM(impression) DESC\"\"\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
