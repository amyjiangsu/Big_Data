{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "```\n",
    "v1.0.0 \n",
    "Benjamin Harder\n",
    "1/5/2016\n",
    "Python 2.7.11\n",
    "Learning Spark\n",
    "```\n",
    "Spark SQL is a extension of Spark which allows for SQL-like queries to be performed against RDDs within memory.  This can be done against native files, or directly to HDFS or HBase - in this case they would be 'HQL-like' queries.  \n",
    "In this example, I will demonstrate interactive data analysis using text files, Spark SQL, dataframes, and functional programming.  Spark SQL jobs will typically involved a few key steps, these include:\n",
    "\n",
    "\n",
    "**Intro Examples** \n",
    "1. Connect to the environment\n",
    "2. Create a Spark Context\n",
    "3. Create a Spark SQL context\n",
    "4. Read in the file to be analyzed\n",
    "5. Transform it into a data frame\n",
    "6. Begin to query that dataframe using Spark SQL.\n",
    "\n",
    "Detailed documentation can be found on the Apache website [here:](http://spark.apache.org/docs/latest/sql-programming-guide.html) - note: this is for Spark 1.5.2\n",
    "\n",
    "Spark process and actions can be viewed at http://localhost:4040/jobs/\n",
    "\n",
    "Lastly - when using Hive the best practice would be to avoid the 4th and 5th steps by connecting to Hive tables directly.  \n",
    "\n",
    "**Advanced Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import getspark #custom script that activates the environment\n",
    "from IPython.display import Image\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext() #Initialize a Spark Context\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with previous examples, we will begin by reading in the text file for the click data.  This version is in CSV format.  After it is read in as an rdd, the next step is to split it by its delimiter, then remove the header and check out the finalized and headerless rdd.  This is a typical data munging step when dealing with RDDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(r\"C:\\Spark\\clickinfo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split it by its delimiter\n",
    "rdd = rdd.map(lambda line: line.split(\",\")) #split it up by comma -transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Strip out the header\n",
    "header = rdd.first() #extract header\n",
    "data = rdd.filter(lambda x:x !=header) #review the headerless rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check out your fancy new rdd\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why go through the trouble?\n",
    "\n",
    "* Schema = table + columns + types\n",
    "* Column names to index\n",
    "* Leverage SQL and relational theory\n",
    "\n",
    "### Why not schemas and SQL?\n",
    "\n",
    "* They make your data structure\n",
    "* Fragility\n",
    "\n",
    "When reviewing our schema, one of the columns has a boolean for male == 1 and female == 0.  To ease interpretability for reporting, we can find a replace the values as male and female respectively.  Note: currently only strings exist in the rdd, in the next step they will be redefined as specific ints, floats, strings etc.  The row function with Spark dataframes is helpful because it allows you to programmatically define the schema up front.  \n",
    "\n",
    "One of the great advantages of Spark is that it is a computing environment and not just a query language.  This means that you can write functions and pass them directly into Spark.  In this case, all of the values with the dataframe are integers which represent factors.  Using Python it is possible to write a collection of functions which loop over the RDD and replace ints with strings.\n",
    "\n",
    "Using the 'map' function loops the code over all of the rows in the RDD.  'Row' tells Spark to assign a header placeholder to the dataframe, along with defining the schema for that particular column.  Finally, 'toDF()' passes the RDD to a dataframe object.  I recommend that RDD is passed to a dataframe at the earliest opportunity, as they are more performant (especially with Python) than operating on standard RDDS.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Instead of reading in the sql context directly, we can define the rows uniquely in the dataframe and define the schema\n",
    "def genmap(gender):\n",
    "    if gender == '0':\n",
    "        return \"Female\"\n",
    "    else:\n",
    "        return \"Male\"\n",
    "    \n",
    "def clickmap(clicks):\n",
    "    if clicks == '0':\n",
    "        return \"No_Click\"\n",
    "    else: \n",
    "        return \"Click\"\n",
    "    \n",
    "def signedin(clicks):\n",
    "    if clicks == '0':\n",
    "        return \"Not_Signed_In\"\n",
    "    else: \n",
    "        return \"Signed_In\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clicksmappd = data.map(lambda line: Row(user_id = str(line[0]), \n",
    "                              clicks = clickmap(line[1]), \n",
    "                              impression=int(line[2]), \n",
    "                              signedin=signedin(line[3]))).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with our dataframe created, we can review the schema and take a topline sample of the dataframe.  Once we have confirmed that the schema matches our desired format the next step is to register a temporary table to be able to we can then execute SQL queries against the data as you would with any normal SQL query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clicksmappd.printSchema() # This maps the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clicksmappd.show(5) # Shows a snippet of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clicksmappd.registerTempTable(\"clickinfo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick review, we have performed all of the necessary operations to get a base csv file into a tabular format to perform Spark SQL queries.  This involved:\n",
    "\n",
    "1. Reading in the base csv file\n",
    "2. Stripping out the header\n",
    "3. Creating functions which looped over the RDD to replace numeric values with strings to improve interpretability.  \n",
    "4. Mapping the headerless RDD to a dataframe\n",
    "5. Registering a temporary table against the dataframe for Spark SQL\n",
    "6. Running and Spark SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"\"\"SELECT clicks, signedin, SUM(impression) as impressions \n",
    "                  FROM clickinfo \n",
    "                  GROUP BY clicks, signedin \n",
    "                  ORDER BY SUM(impression) DESC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will read in the second csv file that contains gender information about the users, and join that to the clicks dataframe using spark sql.  The point of this is to show how joins are performed within Spark SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(r\"C:\\Spark\\userinfo.csv\")\n",
    "rdd = rdd.map(lambda line: line.split(\",\")) #split it up by comma -transformation\n",
    "header = rdd.first() #extract header\n",
    "data = rdd.filter(lambda x:x !=header) #review the headeress rdd\n",
    "usersmappd = data.map(lambda line: Row(user_id = str(line[0]), \n",
    "                                     gender = genmap(line[1]))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usersmappd.registerTempTable(\"userinfo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joindf = sqlContext.sql(\"\"\"SELECT clickinfo.user_id, \n",
    "                    clickinfo.impression, \n",
    "                    clickinfo.clicks, \n",
    "                    clickinfo.signedin, \n",
    "                    userinfo. gender\n",
    "                FROM clickinfo, userinfo\n",
    "                WHERE clickinfo.user_id = userinfo.user_id\n",
    "                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joindf.show(10)\n",
    "joindf.registerTempTable(\"joinedtable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "report = sqlContext.sql(\"\"\"SELECT gender, SUM(impression) as impressions \n",
    "                  FROM joinedtable \n",
    "                  GROUP BY gender \n",
    "                  ORDER BY SUM(impression) DESC\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "report.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to conduct interactive visualizations, it is neccessary to move data from a Spark dataframe to the local machine.  No ways currently exist which allow for visualizations to be performed on large datasets within Spark.  The best way in python to do this is via Pandas and Matplotlib or Seaborne.  the toPandas() function takes the Spark dataframe and moves it into a Pandas dataframe.  It can also be saved as a csv from pandas as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdrept = report.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdrept.to_csv(r\"...Desktop\\FancyReport.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot #Import Matplotlib\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdrept.set_index(['gender']).plot(kind='bar',title=\"Total Impressions By Gender\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
